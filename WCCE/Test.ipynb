{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Test.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPmdrEAn93cbbzCmKeJOVRj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sajjad-Mahmoudi/Thesis/blob/main/Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import h5py\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from sklearn.model_selection import train_test_split\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "import time"
      ],
      "metadata": {
        "id": "JWVvfuvNQt5P"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, Conv2DTranspose, BatchNormalization\n",
        "from keras.layers import Activation, MaxPool2D, Concatenate\n",
        "\n",
        "def conv_block(input, num_filters, kernel_size=(3, 3), activation='relu', **kwargs):\n",
        "    x = Conv2D(num_filters, kernel_size=kernel_size, padding=\"same\", **kwargs)(input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(activation)(x)\n",
        "\n",
        "    x = Conv2D(num_filters, kernel_size=kernel_size, padding=\"same\", **kwargs)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(activation)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def encoder_block(input, num_filters, **kwargs):\n",
        "    s = conv_block(input, num_filters, **kwargs)\n",
        "    p = MaxPool2D((2, 2))(s)\n",
        "    return s, p\n",
        "\n",
        "def decoder_block(input, skip_features, num_filters, kernel_size=(2, 2), **kwargs):\n",
        "    b = Conv2DTranspose(num_filters, kernel_size=kernel_size, strides=2, padding=\"same\", **kwargs)(input)\n",
        "    d = Concatenate()([b, skip_features])\n",
        "    d = conv_block(d, num_filters)\n",
        "    return d\n",
        "\n",
        "def build_unet(input_shape: tuple, num_classes, encoder_filters=None, decoder_filters=None, bridge_filters=1024):\n",
        "    if decoder_filters is None:\n",
        "        decoder_filters = [512, 256, 128, 64]\n",
        "    if encoder_filters is None:\n",
        "        encoder_filters = [64, 128, 256, 512]\n",
        "\n",
        "    inputs = Input(input_shape)  # 160 x 160 x 1\n",
        "\n",
        "    # Encoder Block\n",
        "    s1, p1 = encoder_block(inputs, encoder_filters[0])  # s1: 160 x 160 x 64, p1: 80 x 80 x 64\n",
        "    s2, p2 = encoder_block(p1, encoder_filters[1])  # s2: 80 x 80 x 128, p2: 40 x 40 x 128\n",
        "    s3, p3 = encoder_block(p2, encoder_filters[2])  # s3: 40 x 40 x 256, p3: 20 x 20 x 256\n",
        "    s4, p4 = encoder_block(p3, encoder_filters[3])  # s4: 20 x 20 x 512, p4: 10 x 10 x 512\n",
        "\n",
        "    # Bridge Block\n",
        "    b1 = conv_block(p4, bridge_filters)  # b1: 10 x 10 x 1024\n",
        "\n",
        "    # Decoder Block\n",
        "    d1 = decoder_block(b1, s4, decoder_filters[\n",
        "        0])  # Conv2DTrans: 20 x 20 x 512 => concat: 20 x 20 x 1024 => Conv2D: 20 x 20 x 512\n",
        "    d2 = decoder_block(d1, s3, decoder_filters[\n",
        "        1])  # Conv2DTrans: 40 x 40 x 256 => concat: 40 x 40 x 512 => Conv2D: 40 x 40 x 256\n",
        "    d3 = decoder_block(d2, s2, decoder_filters[\n",
        "        2])  # Conv2DTrans: 80 x 80 x 128 => concat: 80 x 80 x 256 => Conv2D: 80 x 80 x 128\n",
        "    d4 = decoder_block(d3, s1, decoder_filters[\n",
        "        3])  # Conv2DTrans: 160 x 160 x 64 => concat: 160 x 160 x 128 => Conv2D: 160 x 160 x 64\n",
        "\n",
        "    # SoftMax Connection\n",
        "    outputs = Conv2D(num_classes, (1, 1), padding=\"same\", activation=\"softmax\")(d4)  # output: 160 x 160 x 4\n",
        "\n",
        "    model = Model(inputs, outputs, name=\"U-Net\")\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "RqgluJaTXFc2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "c4qRB_6v3cvp"
      },
      "outputs": [],
      "source": [
        "def read_hdf5(file):\n",
        "    with h5py.File(file, 'r') as hf:\n",
        "        data = np.array(hf.get('data'))\n",
        "        label = np.array(hf.get('label'))\n",
        "        return data, label"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def weighted_categorical_crossentropy(weights):\n",
        "    def wcce(y_true, y_pred):\n",
        "        tensor_weight = K.constant(weights, dtype=tf.float32)\n",
        "        y_true = K.cast(y_true, y_pred.dtype)\n",
        "        return K.categorical_crossentropy(y_true, y_pred) * K.sum(y_true * tensor_weight, axis=-1)\n",
        "    return wcce"
      ],
      "metadata": {
        "id": "ERp9RoeLXtaF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data, label = read_hdf5('/content/drive/MyDrive/Thesis/train_preprocessed_patch.h5')"
      ],
      "metadata": {
        "id": "x-1avBhGQTD-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_valid, y_train, y_valid = train_test_split(data, label, test_size=0.2, random_state=3)"
      ],
      "metadata": {
        "id": "ybSsWJMNQynX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h, w, c = data.shape[1], data.shape[2], data.shape[3]\n",
        "model = build_unet((h, w, c), 4)"
      ],
      "metadata": {
        "id": "-4623VJXWxLV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=weighted_categorical_crossentropy([0.27582457, 3.35481053, 14.37382803, 145.83914235]), metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "7L9f5OqGXc9F"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "history = model.fit(x_train, y_train, batch_size=16, verbose=1, epochs=10, validation_data=(x_valid, y_valid), shuffle=False)\n",
        "model.save('/content/drive/MyDrive/Thesis/test_weights.hdf5')\n",
        "\n",
        "seconds = time.time() - start_time\n",
        "print('Time Taken:', time.strftime(\"%H:%M:%S\",time.gmtime(seconds)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyeJee4SYfFW",
        "outputId": "93eb5d27-1ed9-43e2-87eb-a7c3f659d983"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "95/95 [==============================] - 4134s 44s/step - loss: 1.5693 - accuracy: 0.3213 - val_loss: 0.9936 - val_accuracy: 0.8017\n",
            "Epoch 2/10\n",
            "95/95 [==============================] - 4147s 44s/step - loss: 1.6260 - accuracy: 0.3894 - val_loss: 0.9614 - val_accuracy: 0.0283\n",
            "Epoch 3/10\n",
            "95/95 [==============================] - 4170s 44s/step - loss: 1.5094 - accuracy: 0.4580 - val_loss: 0.9608 - val_accuracy: 0.4237\n",
            "Epoch 4/10\n",
            "95/95 [==============================] - 4196s 44s/step - loss: 1.4884 - accuracy: 0.4293 - val_loss: 0.9400 - val_accuracy: 0.6090\n",
            "Epoch 5/10\n",
            "95/95 [==============================] - 4207s 44s/step - loss: 1.5030 - accuracy: 0.4244 - val_loss: 0.9407 - val_accuracy: 0.7026\n",
            "Epoch 6/10\n",
            "95/95 [==============================] - 4216s 44s/step - loss: 1.4851 - accuracy: 0.4319 - val_loss: 0.9483 - val_accuracy: 0.5161\n",
            "Epoch 7/10\n",
            "95/95 [==============================] - 4228s 45s/step - loss: 1.4763 - accuracy: 0.4067 - val_loss: 0.9767 - val_accuracy: 0.0365\n",
            "Epoch 8/10\n",
            "95/95 [==============================] - 4208s 44s/step - loss: 1.4763 - accuracy: 0.4113 - val_loss: 0.9536 - val_accuracy: 0.0224\n",
            "Epoch 9/10\n",
            "95/95 [==============================] - 4202s 44s/step - loss: 1.4708 - accuracy: 0.4203 - val_loss: 0.9779 - val_accuracy: 0.0365\n",
            "Epoch 10/10\n",
            "95/95 [==============================] - 4204s 44s/step - loss: 1.4684 - accuracy: 0.4045 - val_loss: 1.0656 - val_accuracy: 0.0273\n",
            "Time Taken: 11:39:24\n"
          ]
        }
      ]
    }
  ]
}
